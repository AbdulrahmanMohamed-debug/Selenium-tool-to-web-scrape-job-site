import csv
from datetime import date
import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
import json

# set up the webdriver
driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

class SpaceScrape():

    def file_name(self):
        x = date.today()
        return ("SpaceX " + str(x) + ".csv")
    

     #Find the last element on the page by scrolling down
    #have to wait for a second as site loads more jobs
    def scroll(self):    
        
        script = ("window.scrollTo(0, document.body.scrollHeight);"
                  "var lenOfPage=document.body.scrollHeight;"
                  "return lenOfPage;")
        
        lenOfPage = driver.execute_script(script)
        match=False
        while(match==False):
            lastCount = lenOfPage
            time.sleep(1)
            lenOfPage = driver.execute_script(script)

            if lastCount==lenOfPage:
                match=True

            #javascript code scrolls to end of page and returns height of page
            #sets lastCount as lenOfPage
            #waits a second to load more postings
            # runs the script:
            # if there are more jobs then 
                #   lenOfPage will be more than last count
            #else
                # if there are no more jobs loaded lenOfPage will have not changed
                #  and will be equal to lastCount
                #in which case while loop will terminate


    def heads(self):

        titles = {
            'jobTitle'    : "Title_Unavailable",
            'jobLocation' : "Location_Unavailable",
            'date_posted' : "Date_Posted_Unavailable"
        }

        try:
            titles['jobTitle'] = driver.find_element(By.XPATH, "//h1[@class='app-title']").text.replace("\n","")
        except Exception:
            pass

        #the default is the string "...unavailable" above if element is not available    
        
        try:
            titles['jobLocation'] = driver.find_element(By.XPATH, "//div[@class='location']").text.replace("\n","")
        except Exception:
            pass

        
        
        elem = driver.find_element(By.XPATH, "//script[@type='application/ld+json']")
        jsontext = json.loads(elem.get_attribute('innerHTML'))

        try:
            titles['date_posted'] = jsontext['datePosted']
        except Exception:
            pass


        return titles
        
    
    def descriptions(self):

        desc = {
            'Overview'  : "Overview not available",
            'Responsibilities' : "Responsibilities is not availvble",
            'Qualifications'    : "Description not available",
            #'preferred_Qualifications' :  "preferred_Qualifications unavailable"
        }
        
        
        
        # To find expect & do there are two divs with attribute //div[contains(@class,'style_descriptionItem')]
        try:
           desc["Responsibilities"]  = driver.find_elements(By.XPATH, ("//div[@id='content']//ul"))[0].text.replace("\n","")
           #//div[contains(text(), 'date:')]
           #desc["Responsibilities"]  =driver.find_element(By.XPATH, "//p[contains(text(), 'Qualificatins')]/")
        except Exception:
            pass

        try:
            desc["Overview"]    = driver.find_elements(By.XPATH, "//*[@id='content']/descendant::text()")[2].text
            
        except:
            pass

        try:
            desc["Qualifications"]    = driver.find_elements(By.XPATH, "//*/strong[contains(text(), 'BASIC QUALIFICATIONS')]//following::ul")[0].text
            print(desc["Qualifications"])
        except:
            try:
                desc["Qualifications"]    = driver.find_elements(By.XPATH, "//*/b[contains(text(), 'BASIC QUALIFICATIONS')]//following::ul")[0].text
            except:
                pass
        
        return desc
    

    def get_Job_Details(self, job):

        posting = job.get_attribute("href")

        #open a new window
        driver.execute_script("window.open('');")
        window_after = driver.window_handles[1]
        driver.switch_to.window(window_after)

        driver.get(posting)
        headings = self.heads()
        description = self.descriptions()

        #defining a new dictionary that merges both dictionaries: headings & descriptions
        temp = headings
        headings.update(description)
        myDictionary = headings
        headings  = temp
        myDictionary['url_link'] = str(posting)

        return myDictionary

        

    def main(self):
        
        driver.get("https://www.spacex.com/careers/jobs/")
        time.sleep(2)

        #maximise window
        driver.implicitly_wait(4)
        driver.maximize_window()
        self.scroll()
        

        firstPath = "//div[@id='jobs-list']//td/a"
        elements = driver.find_elements(By.XPATH, firstPath) #creates a list of job postings on the landing page

        file_name = self.file_name()
        rowNames = ['jobTitle','jobLocation','date_posted', 'Overview','Responsibilities','Qualifications','url_link']
        with open(file_name, mode='w+', encoding='utf-8', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=rowNames)   
            writer.writeheader()
            for job in elements:
                myDict = self.get_Job_Details(job)        
                writer.writerow(myDict)
                driver.close()
                driver.switch_to.window(driver.window_handles[0])

            driver.quit()


if __name__=="__main__": 
    SpaceScrape().main()

